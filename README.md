# Cliff Walking Q-Learning Agent ğŸ§ ğŸƒ

## ğŸ“Œ Description  
This repository implements a Q-learning agent to solve the Cliff Walking grid-world problem, a classic environment in reinforcement learning.

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ª Q-learning æ™ºèƒ½ä½“ï¼Œç”¨ä»¥è§£å†³ Cliff Walkingï¼ˆæ‚¬å´–è¡Œèµ°ï¼‰é—®é¢˜ï¼Œæ˜¯å¼ºåŒ–å­¦ä¹ ç»å…¸å®éªŒä¹‹ä¸€ã€‚

---

## ğŸ“Š Project Structure / é¡¹ç›®ç»“æ„

| File | Description |
|------|-------------|
| `cliff_walking_q_learning.m` | Main training loop for Q-learning |
| `next_state.m` | Transition logic: from state & action to next state & reward |
| `plot_path.m` | Optional animation for each episode |
| `plot_stoch_pol.m` | Arrow-plot of the policy & most likely path |

---

## ğŸ” Algorithm: Q-learning with Îµ-greedy

- Update Rule:  
   Q(s,a) \leftarrow Q(s,a) + \alpha \left[ R + \gamma \max_a Q(s',a) - Q(s,a) \right] 

- Policy:  
  Îµ-greedy (0.1): mostly exploit, sometimes explore

- Reward Design:  
  - Reaching goal: `-1`  
  - Walking step: `-1`  
  - Falling off cliff: `-100`

---

## ğŸ” Parameters / è¶…å‚æ•°è®¾ç½®

| Parameter | Value | Description |
|----------|--------|-------------|
| `alpha`  | 0.5    | Learning rate |
| `gamma`  | 1.0    | Discount factor |
| `epsilon`| 0.1    | Exploration rate |

---

## ğŸ“ˆ Outputs

- `Returns per Episode` plot
- Learned policy arrows (quiver)
- Shortest path visualization
- Optional animated path per episode

---

## ğŸ“š Reference / å¼•ç”¨æ¥æº

- Sutton & Barto (2018). Reinforcement Learning: An Introduction (Ch. 6: Temporal-Difference Learning)  
  [[Online Book]](http://incompleteideas.net/book/the-book-2nd.html)

---

## âœ… To Run / è¿è¡Œæ–¹å¼

```matlab
% In MATLAB:
cliff_walking_q_learning
